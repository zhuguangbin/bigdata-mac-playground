## Mac基础环境
---
我的Mac系统通常会升级到Mac OS的最新稳定版本，还是相信苹果的稳定性。而且我有用Time Machine定期备份的习惯，一旦有问题，大不了重新恢复。我目前的Mac OS版本为Monterey 12.3.1。原则上，Catalina以及以后的版本应该问题都不大。 

### 软件安装

1. **JDK (必选)** 

不多说，大数据体系尤其是Apache开源组件基本都是围绕Java生态而构建。至于版本，虽然现在Java18都release了，但是Java8仍然是主流和稳定版本。建议你以JDK8为主要版本，如果有必要还可以再安装个JDK11或者更高的版本，多版本并存，按需选用。

到Oracle官方网站自行下载就可以了，`https://www.oracle.com/java/technologies/downloads/#java8-mac`，当然你也可以选择openjdk或者其他发行版本，随你。

2. **iTerm2 (可选)**

Mac自带的Terminal不太好用，尤其是不支持多标签。我推荐你用iTerm2来替代，其丰富的插件、快捷键等，尤其跟oh-my-zsh搭配起来，还可以配置主题，可定制性非常灵活，一定会让你得心应手。

[官网](https://iterm2.com/downloads.html)  下载安装即可。

这里给一篇iTerm2搭配oh-my-zsh的知乎帖子，有空自己去折腾折腾吧。[传送门](https://zhuanlan.zhihu.com/p/290737828)

3. **HomeBrew (必选)** 

HomeBrew强大的包管理功能，丰富了Mac。可以说，有了HomeBrew，你就可以将Mac当成生产Linux环境，常用的各种命令软件，一个brew install 命令即可安装，非常方便。这算是Mac开发者的标配吧，接下来要用到，必须安装。

更多可了解HomeBrew[官网](https://brew.sh/index_zh-cn)，我们直接复制一下命令，在iTerm2或者Terminal中执行安装：
```
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

4. **git/maven/gradle/sbt (必选)** 

源码管理，构建工具，不多说。为什么maven/sbt/gradle都搞？因为都用得到啊。比如hadoop/hive/spark等都是maven构建的，kafka/iceberg是用gradle构建的，sbt虽然慢，但是也有些项目在用，尤其是scala的。spark就既支持maven又支持sbt。
所以，都装了吧，因为有了HomeBrew转起来so easy.

```
brew install git maven gradle sbt
```

5. **docker/minikube (必选)** 

Docker是容器引擎，minikube是Mac上的K8S单机模拟环境。为什么要搞这俩玩意，因为我们他们是云原生的基础啊，我们处在一个大数据+云原生时代嘛。
好了，不务虚了，是因为好多开源组件提供了docker的demo，而且利用Docker的隔离性，我们将简化部署好多与系统依赖性强的组件，而不用适配本地环境，比如MySQL数据库。
而minikube提供一个K8S的本地环境，我们后边的内容用来玩一玩大数据云原生架构，将大数据上云。

a. 去[Docker官网](https://www.docker.com/products/docker-desktop/) 下载个Docker Desktop，傻瓜式安装。安装结束后，启动Docker，启动后可以运行个HelloWorld验证是否成功。

```
docker pull hello-world
docker images
docker run hello-world
```

b. iTerm2或者Terminal中执行以下命令安装minikube和kubectl命令
```
brew install minikube kubectl
```
c. 验证安装

```
minikube start
minikube dashboard
```

6. **VsCode (可选)**

轻量级IDE，用过都说好，而且插件越来越丰富，功能越来越强大，即使你用不到这么多功能，用它来编辑下文本也是好用的，比如笔者此时在写的这个MarkDown的教程就是在用VsCode编辑的。最重要的，免费啊，微软良心大大的。

[官网](https://code.visualstudio.com/download) 下载安装。

7. **Intellij IDEA (可选)**

虽然VsCode轻量级又免费，但是对于Java开发来说，Intellij IDEA还是体验最好的。你可以用Community免费版本，不过要完整的体验其强大功能，还是得Ultimate。
嫌贵？是的，我也买不起，这玩意确实太贵，而且按年付费而不是一次性买断。一年149刀，能不肉疼？那就得发挥我们IT男的智慧了，自己去找找破解方法或者去某宝买个教育邮箱的账号吧。天机不可泄露，不能多说，相信这点事对我们来说都不是困难。

### 基础配置

安装完以上软件，我们需要对我们的Mac进行一下简单的配置，目的是为了更好用。

1. **Docker资源配置**

Docker引擎占用Mac的资源大小，在任务栏点击 Docker Desktop 应用图标 -> Perferences，在左侧导航菜单选择Resources，配置CPU，内存，SWAP和磁盘占用空间大小。
建议CPU/内存不要超过Mac机器配置的一半，Disk Image size决定了你能用多少docker image，根据你的SSD空间大小来定，如果比较紧张，给个32G就够用了。

2. **Docker镜像加速**

国内下载DockerHub镜像太慢了，参考这个[方法](https://yeasy.gitbook.io/docker_practice/install/mirror#macos)，添加百度和网易的国内镜像源。

3. **增加一个/data 挂载点**

为啥要这么搞？因为默认hadoop/zookeeper/kafka等配置中的一些本地目录都放到/tmp下，而有些非常重要的目录，比如NameNode的元数据目录，比如Kafka的log.dirs（日志存放目录），都是非常重要的数据，
我们肯定不希望放到/tmp下Mac重启后这些数据就丢失，重新搞Hadoop/Kafka集群。这些比较重要的、又代表我们本地集群核心数据的目录，我们统一规划到/data下，如下：

```
$ ls -l /data/
total 0
drwxr-xr-x   4 guangzhu  wheel  128 Apr 28 22:02 dfs
drwxr-xr-x   4 guangzhu  wheel  128 Nov 30 14:55 hadoop
drwxr-xr-x   4 guangzhu  wheel  128 Apr 28 22:03 hive
drwxr-xr-x  10 guangzhu  wheel  320 May  1 19:46 kafka
drwxr-xr-x   5 guangzhu  wheel  160 Apr 30 17:32 presto
drwxr-xr-x   5 guangzhu  wheel  160 Apr 28 22:03 spark
drwxr-xr-x   3 guangzhu  wheel   96 Apr 30 16:44 zookeeper
```

由于Catalina后对于根目录的写保护，Mac OS下是没办法创建/data目录的，假如你执行`sudo mkdir /data` 会报`Read-only file system`的错误。

实际上未必要/data目录，你当然可以规划放到你自己的Home目录的任意路径下，比如`/Users/{Your Name}/data`。
但是我个人有洁癖，还是希望这个目录跟用户名无关，而且我们原来线上环境也是这么规划的，我希望我的Mac也遵循相同的规范。那这个权限问题怎么解呢？

Mac OS提供了一个`synthetic.conf`的机制，用于配置挂载点。我的理解有点类似于Linux系统的 `/etc/fstab`。允许你配置一个挂载点，挂载到磁盘的某个位置。
关于`synthetic.conf`，你可以`man synthetic.conf`来详细了解下，或者看看这篇知乎[帖子](https://zhuanlan.zhihu.com/p/87601725)吧，这里直接给出命令，供你直接操作。

 a. 首先vim打开`/etc/synthetic.conf`:
 ```
 sudo vim /etc/synthetic.conf
 ```
 添加以下内容：
 ```
 data    /System/Volumes/Data/data
 ```
注意中间是tab，不要被vim用空格来转义，否则无法生效。

b. 在`/System/Volumes/Data/` 下创建实际的data目录，并给他777权限吧
```
cd /System/Volumes/Data
sudo mkdir data
sudo chmod -R 777 data
```

c. 重启Mac生效

d. 初始化一些子目录（后续安装部署各模块会用得到）

```
mkdir -p /data/{hadoop,hive,spark}/{logs,tmp}
mkdir -p /data/dfs/{nn,dn}
mkdir -p /data/{zookeeper,kafka}
```

4. **/opt 目录**

我们统一规划将hadoop/hive/spark/kafka/flink/presto等等这些安装包部署在`/opt`下，加入你的Mac上没有这个目录，参考上一节`/data`增加一个`/opt`的挂载点。

以下是我的`/opt`目录下安装的各个组件，接下来我们将一一讲解如何配置部署。

```
$ cd /opt 
cd /opt

# guangzhu @ C02GM22AMD6T in /opt [23:37:10] 
$ ls -l 
ls -l 
total 1324400
drwxr-xr-x  15 guangzhu  staff        480 Sep 17  2020 alluxio-2.3.0
drwxr-xr-x  13 guangzhu  staff        416 Apr 23 18:42 apache-dolphinscheduler-3.0.0-alpha-bin
-rwxrwxrwx   1 guangzhu  staff  678088937 Apr 23 17:10 apache-dolphinscheduler-3.0.0-alpha-bin.tar.gz
drwxr-xr-x  13 guangzhu  staff        416 Sep  3  2020 apache-druid-0.19.0
drwxr-xr-x  12 guangzhu  staff        384 Jul 23  2021 apache-hive-1.1.0-bin
drwxr-xr-x  12 guangzhu  staff        384 Aug 20  2021 apache-hive-1.2.1-bin
drwxr-xr-x  14 guangzhu  staff        448 Apr 28 23:32 apache-hive-2.3.6-bin
drwxr-xr-x  13 guangzhu  staff        416 Jul  9  2021 apache-hive-3.1.2-bin
drwxr-xr-x  17 guangzhu  staff        544 Apr 11 20:40 apache-kyuubi-1.5.0-incubating-bin
drwxr-xr-x  10 guangzhu  staff        320 Sep 13  2021 azkaban-exec-server-4.0.0-qihoo.2
drwxr-xr-x   9 guangzhu  staff        288 Sep 13  2021 azkaban-web-server-4.0.0-qihoo.2
drwxr-xr-x   3 guangzhu  staff         96 Jan  4  2021 clickhouse
drwxr-xr-x   5 guangzhu  staff        160 Sep  5  2020 docker-druid
lrwxr-xr-x   1 guangzhu  staff         23 Apr 11 15:52 flink -> flink-1.14.4_scala_2.12
drwxr-xr-x  14 guangzhu  staff        448 Nov 18 18:04 flink-1.13.2_scala_2.12
drwxr-xr-x  14 guangzhu  staff        448 Mar  1 11:48 flink-1.14.3_scala_2.11
drwxr-xr-x  14 guangzhu  staff        448 Apr 11 10:04 flink-1.14.4_scala_2.12
lrwxr-xr-x   1 guangzhu  staff         30 Apr 11 10:40 flink-sql-gateway -> flink-sql-gateway-0.4-SNAPSHOT
drwxr-xr-x   5 guangzhu  staff        160 Feb 24 14:37 flink-sql-gateway-0.4-SNAPSHOT
lrwxr-xr-x   1 guangzhu  staff         12 Apr 17  2020 hadoop -> hadoop-2.7.5
drwxr-xr-x  13 guangzhu  staff        416 Apr  8 10:17 hadoop-2.7.5
drwxr-xr-x  12 guangzhu  staff        384 Jun  7  2020 hadoop-3.2.1
lrwxr-xr-x   1 guangzhu  staff         12 Aug  6  2020 hbase -> hbase-1.4.13
drwxr-xr-x  11 guangzhu  staff        352 Aug  6  2020 hbase-1.4.13
drwxr-xr-x  13 guangzhu  staff        416 Nov 16  2020 hbase-2.2.1
lrwxr-xr-x   1 guangzhu  staff         21 Apr 11 10:40 hive -> apache-hive-2.3.6-bin
lrwxr-xr-x   1 root      wheel         64 May  4 19:43 jdk8 -> /Library/Java/JavaVirtualMachines/jdk1.8.0_331.jdk/Contents/Home
lrwxr-xr-x   1 guangzhu  staff         16 Apr 24  2020 kafka -> kafka_2.11-2.4.1
drwxr-xr-x   9 guangzhu  staff        288 Apr 30 16:04 kafka_2.11-2.4.1
lrwxr-xr-x   1 root      wheel         42 Apr 30 20:38 keddah -> /Users/guangzhu/Code/adgit/big-data/keddah
lrwxr-xr-x   1 guangzhu  staff         34 Apr 11 15:54 kyuubi -> apache-kyuubi-1.5.0-incubating-bin
drwxr-xr-x  24 guangzhu  staff        768 Jun  2  2017 pig-0.17.0
lrwxr-xr-x   1 guangzhu  staff         10 Sep 11  2019 play -> play-2.2.6
drwxrwxr-x   9 guangzhu  staff        288 Jun  7  2020 play-2.2.6
lrwxr-xr-x   1 root      wheel         27 Apr 30 17:05 presto -> presto-server-0.240-qihoo.1
drwxr-xr-x   8 guangzhu  staff        256 Nov 26 11:35 presto-server-0.240-qihoo.1
drwxr-xr-x  18 guangzhu  staff        576 Feb  7  2021 spark-2.4.5-bin-hadoop2.7
drwxr-xr-x  18 guangzhu  staff        576 Apr 30 17:34 spark-3.1.2-bin-hadoop2.7
drwxr-xr-x  17 guangzhu  staff        544 May 24  2021 spark-3.1.2-bin-hadoop3.2
lrwxr-xr-x   1 guangzhu  staff         25 Aug 17  2021 spark2 -> spark-2.4.5-bin-hadoop2.7
lrwxr-xr-x   1 guangzhu  staff         25 Aug  9  2021 spark3 -> spark-3.1.2-bin-hadoop2.7
drwxr-xr-x  15 guangzhu  staff        480 Jan 25  2021 ultron


```

5. **SSH配置**

我们需要配置SSH连接本机无密码登陆，比如hadoop/spark等集群启动时需要用。这一步不是必要的，但是为了方便还是建议配置好。


a. 开启Mac的Remote Login

在Mac的系统配置——>共享——>勾选“远程登录”的选项

b. 配置无密码SSH登录本机(localhost)

首先通过以下命令，先生成一份ssh公私钥，如果已经生成过则跳过这步
```
$ ssh-keygen
```
接下来，通过以下命令配置SSH无密码登录本机
```
$ ssh-copy-id localhost
```

你可以通过以下命令来验证是否成功
```
$ssh localhost
```
如果不需要输入密码即可登录，则配置成功。



至此，Mac的基本环境配置完成。接下来，我们就开始我们的大数据游乐场部署吧。