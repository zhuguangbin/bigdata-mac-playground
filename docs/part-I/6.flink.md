## Flink

1. 下载Flink安装包

我们选择Flink的1.14.4版本部署。实际上目前Flink最新版本1.15.0，由于刚发布，好多生态的集成还没有ready，而且第一个小版本不建议生产应用。当然你想本地测试和尝试新功能，可以early bird玩一下。

```
sudo wget -P /opt https://archive.apache.org/dist/flink/flink-1.14.4/flink-1.14.4-bin-scala_2.12.tgz
cd /opt
sudo tar zxvf flink-1.14.4-bin-scala_2.12.tgz
sudo ln -nsf flink-1.14.4_scala_2.12 flink
```

安装可选的format/connector：
```
cd /opt/flink
mkdir ext-lib
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-avro/1.14.4/flink-sql-avro-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-orc_2.12/1.14.4/flink-sql-orc_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-parquet_2.12/1.14.4/flink-sql-parquet_2.12-1.14.4.jar

sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.12/1.14.4/flink-sql-connector-kafka_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.6_2.12/1.14.4/flink-sql-connector-hive-2.3.6_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hbase-2.2_2.12/1.14.4/flink-sql-connector-hbase-2.2_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch7_2.12/1.14.4/flink-sql-connector-elasticsearch7_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc_2.12/1.14.4/flink-connector-jdbc_2.12-1.14.4.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.18/mysql-connector-java-8.0.18.jar
sudo wget -P /opt/flink/ext-lib https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar

```

2. 配置Flink的配置文件

我把我的配置文件放到[conf/flink](../../conf/flink/) 下，供你直接可用。

```
sudo mkdir /etc/flink
cd /etc/flink
sudo ln -nsf ~/Code/github/zhuguangbin/bigdata-mac-playground/conf/flink/conf.local conf
```

3. 环境变量

 将以下环境变量加入`~/.zshrc`

```
export FLINK_HOME=/opt/flink
export FLINK_CONF_DIR=/etc/flink/conf
export PATH=$PATH:$FLINK_HOME/bin

alias flink-sql='$FLINK_HOME/bin/sql-client.sh embedded -i /etc/flink/conf/init.sql -l $FLINK_HOME/ext-lib'
# for flink sql client remote debug
#export JVM_ARGS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5004"
```

`source ~/.zshrc`生效

4. 启动Flink集群

Flink Session Cluster支持多种部署模式：

a. Standalone Session Cluster

```
$ cd /opt/flink

$ bin/start-cluster.sh 
Starting cluster.
Starting standalonesession daemon on host C02GM22AMD6T.
Starting taskexecutor daemon on host C02GM22AMD6T.

```

浏览器访问`http://localhost:8081/` 打开Flink的WebUI。

b. Yarn Session Cluster

```
$ cd /opt/flink
$ bin/yarn-session.sh -h

$ bin/yarn-session.sh --queue default --jobManagerMemory 1GB --taskManagerMemory 1GB --slots 2
```

当看到显示`JobManager Web Interface: http://localhost:XXXXX`时，集群启动成功，可以根据提示的地址在浏览器打开WebUI。或者通过YARN ResourceManager的UI，找到对应的application，通过Tracking UI打开。

c. Kubernetes Session Cluster

K8S的环境稍微有点复杂，我们后续单独开一篇单独介绍。

给大家官方的文档，有兴趣的可以深入了解下这几种集群部署模式：

* [Standalone](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/resource-providers/standalone/overview/)

* [Yarn](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/resource-providers/yarn/)

* [Native K8s](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/resource-providers/yarn/)

5. Flink 运行Job初体验

Flink可以将Job提交到Session Cluster，还支持perjob和application模式。

我们这里为了简单，Flink Job用remote方式提交到Standalone集群的模式。

```
$ flink run examples/streaming/WordCount.jar 
$ flink run examples/batch/WordCount.jar 
$ flink run examples/table/WordCountSQLExample.jar
```

Flink客户端的完整使用，可参考[官方文档](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/cli/)

6. FlinkSQL 初探

Flink的SQL语义已经逐步完善，对于实时数仓开发来说，用SQL无论是开发效率还是可治理性上都比写API更好。所以我们重点介绍下FlinkSQL的使用。

如第3部分配置环境变量时，我们定义了一个`flink-sql`的alias，我们可以直接在iTerm2中输入`flink-sql`即可进入FlinkSQL Client。

关于`flink-sql`的参数中的`/etc/flink/conf/init.sql`初始化脚本非常重要，用于启动FlinkSQL客户端时的基本环境配置，比如初始化HiveCatalog、Module，配置常用的参数等。有兴趣的可以打开研究下，我们直接开始：

```
$ flink-sql

Flink SQL> show catalogs;
+-----------------+
|    catalog name |
+-----------------+
| default_catalog |
|      local_hive |
+-----------------+
2 rows in set

Flink SQL> use catalog local_hive;
[INFO] Execute statement succeed.

Flink SQL> show databases;
+--------------------+
|      database name |
+--------------------+
|            default |
|               test |
+--------------------+
2 rows in set

Flink SQL> create database flink_rt_db;
[INFO] Execute statement succeed.

Flink SQL> show databases;
+--------------------+
|      database name |
+--------------------+
|            default |
|        flink_rt_db |
|               test |
+--------------------+
3 rows in set

Flink SQL> use flink_rt_db;
[INFO] Execute statement succeed.


Flink SQL> CREATE TABLE Orders (
>     order_number BIGINT,
>     price        DECIMAL(32,2),
>     buyer        ROW<first_name STRING, last_name STRING>,
>     order_time   TIMESTAMP(3)
> ) WITH (
>   'connector' = 'datagen'
> );
[INFO] Execute statement succeed.

Flink SQL> show tables;
+-------------------------------------+
|                          table name |
+-------------------------------------+
|                              orders |
+-------------------------------------+
1 rows in set

Flink SQL> describe orders;
+--------------+----------------------------------------------+------+-----+--------+-----------+
|         name |                                         type | null | key | extras | watermark |
+--------------+----------------------------------------------+------+-----+--------+-----------+
| order_number |                                       BIGINT | true |     |        |           |
|        price |                               DECIMAL(32, 2) | true |     |        |           |
|        buyer | ROW<`first_name` STRING, `last_name` STRING> | true |     |        |           |
|   order_time |                                 TIMESTAMP(3) | true |     |        |           |
+--------------+----------------------------------------------+------+-----+--------+-----------+
4 rows in set

Flink SQL> select * from orders;
+----+----------------------+------------------------------------+--------------------------------+-------------------------+
| op |         order_number |                              price |                          buyer |              order_time |
+----+----------------------+------------------------------------+--------------------------------+-------------------------+
| +I |  5297740869456178168 |  450768749399392972185729499136.00 | +I[1cdea31d7ab6d1e9fca9ba89... | 2022-05-19 09:39:51.901 |
| +I |  6838199745008958692 |  644988336437480633151732580352.00 | +I[72c24e2676cdb2e99a30231b... | 2022-05-19 09:39:51.901 |
| +I | -9000180541622383742 |  430067697949619332680008073216.00 | +I[d60dcf6fea3987accf4f2e0d... | 2022-05-19 09:39:51.901 |
| +I | -5612172541236103595 |   56698672101396455833275990016.00 | +I[7cac7ac6b7d926db67ccefdc... | 2022-05-19 09:39:51.901 |
| +I |   803480318937738247 |   51961883839676485025560264704.00 | +I[708d2c900eb94b1ae5652470... | 2022-05-19 09:39:51.901 |
| +I | -2254124079772537754 |  257380533130255119599645229056.00 | +I[ea04b05fe7f671b0a1546b5c... | 2022-05-19 09:39:51.901 |
| +I | -2635649360330740092 |  682799043558527440405871460352.00 | +I[56824ba8b4c3af4a16ea2344... | 2022-05-19 09:39:51.902 |

......

```

以上，我们简单体验了下FlinkSQL（主要是streaming模式），如果你想深入学习FlinkSQL，我建议参考官方的[CookBook](https://github.com/ververica/flink-sql-cookbook)。

这个CookBook依赖一个flink-faker的connector，你可以理解为一个更强大的datagen，用来模拟生成测试数据。

flink-faker项目的jar并没有发布到maven中央仓库，你需要自己clone源码编译安装。flink-faker项目的github地址在`https://github.com/knaufk/flink-faker/tree/flink114`。

```
git clone https://github.com/knaufk/flink-faker.git
cd flink-faker
git checkout v0.4.1
mvn clean package
cp target/flink-faker-0.4.1.jar /opt/flink/ext-lib/
```

退出flink-sql客户端重新进入后，我们专门建一个`flink_sql_cookbook`的db来学习FlinkSQL。

```
Flink SQL> use catalog local_hive;
[INFO] Execute statement succeed.

Flink SQL> create database flink_sql_cookbook;
[INFO] Execute statement succeed.

Flink SQL> use flink_sql_cookbook;
[INFO] Execute statement succeed.

```

好了，接下来，按照文档开始你的学习吧。Flink的概念有点复杂，我建议你在学习之前，先学习下官方的文档，了解下Flink的基本概念，尤其是以下几个文档：

* [动态表的概念](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/concepts/dynamic_tables/)

* [SQL语法](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/sql/gettingstarted/)

* [支持的连接器](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/overview/)

以上就是Flink安装部署的全部内容，后续，我们将根据实例结合Kafka/Hive/MySQL等介绍FlinkSQL在实时数仓中的应用。