## SparkSQL支持多Hive数据源联邦查询

在[这篇](../part-I/4.spark.md)中我们已经介绍过Spark的安装部署以及用SparkSQL处理Hive数据，本篇我们介绍一个SparkSQL上的功能增强：多Hive数据源联邦。

### 背景与介绍

大家都知道Spark处理Hive数据做ETL现在几乎是事实标准。Hive作为Metastore做数仓的元数据，Spark作为ETL引擎。但是Spark只能对接一个Hive，在某些场景下我们有需求将多个Hive数据源做联邦查询或者跨Hive源处理数据。

官方Spark目前最新版本3.2.1也没有完善这部分功能。从底层实现上来说，目前Spark默认只支持一个DataSource。从SQL语义上来说，Spark一开始的设计就没有考虑多Catalog的情况，这点上不像Presto/Flink等，Catalog是一等公民，一开始架构设计就是connector模式来支持多数据源。

在我们的场景中，这个需求其实还是很强烈的。比如以下场景：

1. Hive版本平滑升级

原数仓在Hive1.2版本上，已经运行了多年，随着技术架构的升级演进，需要升级Hive到新版本，比如2.3，因为性能、因为Hudi/Iceberg数据湖的支持等原因。但是直接原地升级，涉及的影响巨大，需要Infra Team前期做全部case的测试，并组织所有的业务线在升级时刻Standby，一旦有问题，回滚后，得全部重跑受影响的Pipeline。
另一个方面，老的数仓当初建设时，并没有做的很规范。比如一些数据组织方式，比如权限，比如一些表和字段的命名规范，这些数据治理层面的事情，老的数仓存在很大的历史包袱，而这些可以在Hive 2.3新数仓上重新建设。这样的规划下，就必须要求同一个Spark Job能够既能在新Hive上处理，又能访问老的Hive。
随着新数仓的逐步规范和强大，老的数仓数据逐步被废弃或者迁移过来，这样的升级路线虽然长，但是是更符合实际和对业务平滑的。

2. 多Hive的联邦查询处理

公司内部有多个IDC的不同Hive分别在独立的Hadoop集群上，而且版本不一。有这种需求，将IDC-A的Hive中的某些表同步到IDC-B的Hive中，或者同一个IDC下的两个Hive数据源（如上述的新老两个数仓）做联邦查询比如UNION/JOIN等。如果不能在一个Spark Job中处理，只能是独立处理后，将结果distcp到本地Hive然后创建外表后继续处理。
而在同一个Spark Job中，不同IDC的Hive表，或者同IDC的多个Hive表，可以在一个SQL中完成处理，大大降低了ETL pipeline的复杂度。

我们调研了Spark的源码，发现其Catalog可以以插件的方式进行扩展，只需要继承`TableCatalog`接口实现一个`V2ExternalCatalog`，而其内部实现遵循DataSourceV2的相应接口。在SQL语法层面上，我们通过extension扩展parser和command，来支持CATALOG语义。

整个扩展以spark plugin的方式，并不直接修改Spark源码，使其可插拔，避免侵入。

项目源码我放到github上了，地址在 https://github.com/zhuguangbin/spark-plugins。


### 主要功能

1. 多Hive数据源连接

支持多个Hive数据源挂载的方式，并且各Hive数据源支持不同版本。

2. SparkSQL语法增强，支持CATALOG语义

我们增强了SparkSQL语法，参考Flink/Presto等，让其天然支持CATALOG语义。一个CATALOG即为一个Hive数据源（Metastore），支持以下语法：

```
SHOW CATALOGS
USE CATALOG <catalog name>
```

3. 插件式安装，支持最新的Spark版本，仅需增加配置，无需修改源码

我们尽量不修改Spark的源码来增加侵入性和维护负担，以插件的方式，用户只需要简单配置即可使用。默认我们是以Spark 3.1.2来build的，原则上支持Spark 3的所有版本。（3.0.x/3.2.x 没有测试过，你可以修改pom的依赖重新build后使用）


### 配置使用

1. 编译

```
mvn clean package
```
编译成功后，在`spark-extensions/spark-extension-addon/target/`下找到`spark-extension-addon-0.1-SNAPSHOT.jar`。

我们将其他部署到`/data/spark/lib`下：

```
$ mkdir /data/spark/lib/
$ cp target/spark-extension-addon-0.1-SNAPSHOT.jar /data/spark/lib 
```

2. Spark配置

我在我本地启动了另一个Hive 1.2.1版本的Hive Metastore 与原来的 Hive 2.3.6的metastore共存，模拟两个Hive数据源。

Hive 2.3.6是默认的Hive数据源，我们在介绍第三节时已经部署，请回顾参考。新部署Hive 1.2.1版本并启动metastore，这部分的部署步骤我放到附录中。

接下来，我们只需要更新`spark-defaults.conf`配置即可：

```
# use AddonSparkSQLExtension to extend spark sql capacity
spark.sql.extensions                            cn.com.bigdata123.spark.sql.AddonSparkSQLExtension

# add extension jar to driver classpath and yarn dist jar
spark.driver.extraClassPath        /data/spark/lib/spark-extension-addon-0.1-SNAPSHOT.jar
spark.yarn.dist.jars               file:///data/spark/lib/spark-extension-addon-0.1-SNAPSHOT.jar

# default catalog
spark.sql.defaultCatalog			spark_catalog

# default session catalog configuration
spark.sql.hive.metastore.version                2.3.7
spark.sql.hive.metastore.jars                   builtin 

# local hive 1.2.1 as external catalog, which named local_hive1
spark.sql.catalog.local_hive1						cn.com.bigdata123.spark.plugin.catalog.hive.V2ExternalCatalog
# put local hive 1.2.1 hive-site.xml to SPARK_CONF_DIR, rename it to hive-site.{CATALOG_NAME}.xml
spark.sql.catalog.local_hive1.hive-site-file				hive-site.local_hive1.xml
spark.sql.catalog.local_hive1.spark.sql.hive.metastore.version  		1.2.1
spark.sql.catalog.local_hive1.spark.sql.hive.metastore.jars             	path
spark.sql.catalog.local_hive1.spark.sql.hive.metastore.jars.path        	file:///opt/hive-1.2.1/lib/*.jar,file:///opt/hadoop/share/hadoop/common/*.jar,file:///opt/hadoop/share/hadoop/common/lib/*.jar,file:///opt/hadoop/share/hadoop/hdfs/*.jar,file:///opt/hadoop/share/hadoop/hdfs/lib/*.jar,file:///opt/hadoop/share/hadoop/yarn/*.jar,file:///opt/hadoop/share/hadoop/yarn/lib/*.jar,file:///opt/hadoop/share/hadoop/mapreduce/*.jar,file:///opt/hadoop/share/hadoop/mapreduce/lib/*.jar
# you can build an assemble jar and put it to hdfs instead of a bunch of local jar files
#spark.sql.catalog.local_hive1.spark.sql.hive.metastore.jars.path		hdfs://localhost:8020/data/spark/lib/hive-metastore-libs/hive-metastore-libs-1.2.1-assemble.jar

```

完整的配置文件我更新到[conf/spark](../../conf/spark/) 下了，你可以直接拿来使用。 

### Demo

按照以上部署配置完成后，我们来demo测试验证下

```
$ spark-sql 
22/05/20 12:27:55 WARN Utils: Your hostname, C02GM22AMD6T resolves to a loopback address: 127.0.0.1; using 192.168.1.40 instead (on interface en0)
22/05/20 12:27:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/05/20 12:27:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark master: yarn, Application Id: application_1653020495427_0004
// default session catalog
spark-sql> show databases;
default
hive_db
hudi_db
iceberg_db
test
Time taken: 1.763 seconds, Fetched 5 row(s)
spark-sql> use hive_db;
Time taken: 0.053 seconds
spark-sql> show tables;
Time taken: 0.071 seconds
spark-sql> use test;
Time taken: 0.024 seconds
spark-sql> show tables;
test    hello_hive    false
test    hello_spark    false
Time taken: 0.062 seconds, Fetched 2 row(s)

// show all mounted catalogs
spark-sql> show catalogs;
spark_catalog
local_hive1
Time taken: 0.022 seconds, Fetched 2 row(s)

// change to local_hive1 catalog
spark-sql> use catalog local_hive1;
Time taken: 0.033 seconds
spark-sql> show databases;
default
test1
Time taken: 0.585 seconds, Fetched 2 row(s)
spark-sql> use test1;
Time taken: 0.026 seconds
spark-sql> show tables;
Time taken: 0.053 seconds

// create a target table in hive1(local_hive1 catalog) which data if computed from hive2 (spark_catalog catalog)
spark-sql> create table hello_hive1_table as select * from spark_catalog.test.hello_hive;
22/05/20 12:30:03 WARN V2ExternalCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22/05/20 12:30:04 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
Time taken: 5.519 seconds
spark-sql> select * from hello_hive1_table;
1    Bob
2    Alice
3    Lucy
Time taken: 0.632 seconds, Fetched 3 row(s)

// change back to spark_catalog (default catalog)
spark-sql> use catalog spark_catalog;
Time taken: 0.097 seconds
spark-sql> use test;
Time taken: 0.021 seconds
spark-sql> show tables;
test    hello_hive    false
test    hello_spark    false
Time taken: 0.043 seconds, Fetched 2 row(s)

// create a target table in hive2(default catalog) which data is computed from hive1 (local_hive1 catalog)
spark-sql> create table hello_hive2_table as select * from local_hive1.test1.hello_hive1_table;
22/05/20 12:31:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22/05/20 12:31:09 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
Time taken: 1.312 seconds
spark-sql> set spark.sql.legacy.createHiveTableByDefault=false;
spark.sql.legacy.createHiveTableByDefault    false
Time taken: 0.022 seconds, Fetched 1 row(s)
spark-sql> create table hello_hive2_table as select * from local_hive1.test1.hello_hive1_table;
Error in query: Table test.hello_hive2_table already exists. You need to drop it first.
spark-sql> drop table hello_hive2_table;
Time taken: 0.464 seconds
spark-sql> create table hello_hive2_table as select * from local_hive1.test1.hello_hive1_table;
Time taken: 1.301 seconds
spark-sql> show tables;
test    hello_hive    false
test    hello_hive2_table    false
test    hello_spark    false
Time taken: 0.039 seconds, Fetched 3 row(s)
spark-sql> select * from hello_hive2_table;
1    Bob
2    Alice
3    Lucy
Time taken: 0.31 seconds, Fetched 3 row(s)
spark-sql> 

// see which catalog and database you are now in
spark-sql> show current namespace;
spark_catalog    test
Time taken: 0.033 seconds, Fetched 1 row(s)

// change to another catalog, see it again
spark-sql> use catalog local_hive1;
Time taken: 0.02 seconds
spark-sql> show current namespace;
local_hive1    default
Time taken: 0.014 seconds, Fetched 1 row(s)
spark-sql> use test1;
Time taken: 0.018 seconds
spark-sql> show current namespace;
local_hive1    test1
Time taken: 0.016 seconds, Fetched 1 row(s)
spark-sql> 

// union the two hive datasource in a query
spark-sql> select * from (select * from spark_catalog.test.hello_hive2_table union all select * from local_hive1.test1.hello_hive1_table);
1    Bob
2    Alice
3    Lucy
1    Bob
2    Alice
3    Lucy
Time taken: 2.893 seconds, Fetched 6 row(s)
spark-sql>

// explore more，like join etc.
```

好了，如上所见，我们使用表时，仅需要写全`catalog.database.table`，利用这样的multipartIdentifier来定位多个hive数据源的表，就像使用同Hive的其他表一样，其他语法完全不变，是不是很方便。


### 附： 多版本Hive Metastore部署

我们在第三节的基础上，再部署一个Hive 1.2.1版本，并启动另一个metastore。

1. Hive安装包

```
sudo wget -P /opt/ https://archive.apache.org/dist/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
cd /opt
sudo tar zxvf apache-hive-1.2.1-bin.tar.gz
sudo ln -nsf apache-hive-1.2.1-bin hive-1.2.1

sudo wget -P /opt/hive-1.2.1/lib/ https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar
```

2. mysql 初始化

```
$ mysql -h 127.0.0.1 -u root -p                                                                
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 240
Server version: 5.7.38 MySQL Community Server (GPL)

Copyright (c) 2000, 2022, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> create database hive1_meta;
Query OK, 1 row affected (0.01 sec)

mysql> exit;
Bye


# guangzhu @ C02GM22AMD6T in /opt [11:14:59] 
$ cd /opt/hive-1.2.1

# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [11:15:07] 
$ cd scripts/metastore/upgrade/mysql 

# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1/scripts/metastore/upgrade/mysql [11:15:23] 
$ mysql -h 127.0.0.1 -u root -p hive1_meta < hive-schema-1.2.0.mysql.sql 
Enter password: 

# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1/scripts/metastore/upgrade/mysql [11:15:54] 
$ mysql -h 127.0.0.1 -u root -p hive1_meta                              
Enter password: 
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 242
Server version: 5.7.38 MySQL Community Server (GPL)

Copyright (c) 2000, 2022, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show tables;
+---------------------------+
| Tables_in_hive1_meta      |
+---------------------------+
| BUCKETING_COLS            |
| CDS                       |
| COLUMNS_V2                |
| COMPACTION_QUEUE          |
| COMPLETED_TXN_COMPONENTS  |
| DATABASE_PARAMS           |
| DBS                       |
| DB_PRIVS                  |
| DELEGATION_TOKENS         |
| FUNCS                     |
| FUNC_RU                   |
| GLOBAL_PRIVS              |
| HIVE_LOCKS                |
| IDXS                      |

```

3. Hive配置

我已经将1.2的hive 的配置放到[conf/hive/conf.local-1.2](../../conf/hive/conf.local-1.2)下了。

```
$ sudo ln -nsf ~/Code/github/zhuguangbin/bigdata-mac-playground/conf/hive/conf.local-1.2 conf-1.2

# guangzhu @ C02GM22AMD6T in /etc/hive [11:21:49] 
$ ls -l
total 0
lrwxr-xr-x  1 root  wheel  69 Apr 28 22:07 conf -> /Users/guangzhu/github/zhuguangbin/bigdata-mac-playground/conf/hive/conf.local
lrwxr-xr-x  1 root  wheel  73 May 20 11:21 conf-1.2 -> /Users/guangzhu/github/zhuguangbin/bigdata-mac-playground/conf/hive/conf.local-1.2

```

4. 启动Hive Metastore

```
# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [12:00:59] 
$ export HIVE_HOME=/opt/hive-1.2.1                         

# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [12:01:02] 
$ export HIVE_CONF_DIR=/etc/hive/conf-1.2             

# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [12:01:04] 
$ bin/hive --service metastore -p 9082                
ls: /opt/spark3/lib/spark-assembly-*.jar: No such file or directory
Starting Hive Metastore Server
^C%                                                                                                                                                              
# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [12:01:19] C:130
$ nohup bin/hive --service metastore -p 9082 > /data/hive/logs/hive1-metastore.out &
[1] 70422
```

5. 测试验证

```
# guangzhu @ C02GM22AMD6T in /opt/hive-1.2.1 [12:02:53] C:1
$ cd /opt/hadoop

# guangzhu @ C02GM22AMD6T in /opt/hadoop [12:03:07] 
$ sbin/start-dfs.sh 
Starting namenodes on [localhost]
localhost: starting namenode, logging to /opt/hadoop-2.7.5/logs/hadoop-guangzhu-namenode-C02GM22AMD6T.out
localhost: starting datanode, logging to /opt/hadoop-2.7.5/logs/hadoop-guangzhu-datanode-C02GM22AMD6T.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.7.5/logs/hadoop-guangzhu-secondarynamenode-C02GM22AMD6T.out

# guangzhu @ C02GM22AMD6T in /opt/hadoop [12:03:30] 
$ cd /opt/hive-1.2.1

$ bin/hive
ls: /opt/spark3/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in jar:file:/opt/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> show databases;
OK
default
Time taken: 0.833 seconds, Fetched: 1 row(s)
hive> create database test1;
OK
Time taken: 0.592 seconds
hive> describe database test1;
OK
test1        hdfs://localhost:8020/user/hive/warehouse1/test1.db    guangzhu    USER    
Time taken: 0.087 seconds, Fetched: 1 row(s)
hive> exit;
```